"""
Streaming LLM module for token-by-token streaming response generation.

This module provides classes for working with streaming responses from language models.
"""

import asyncio
import json
import logging
import httpx
from typing import Dict, List, Optional, AsyncGenerator, Any, Union
import os
from pydantic import BaseModel

# Setup logger
logger = logging.getLogger(__name__)

class StreamingResponse(BaseModel):
    """
    Model for streaming responses.
    
    Attributes:
        tokens: The list of tokens generated so far
        text: The full text generated by concatenating all tokens
        sources: Optional list of sources used to generate the response
    """
    tokens: List[str] = []
    text: str = ""
    sources: Optional[List[Dict[str, Any]]] = None
    
    def add_token(self, token: str) -> None:
        """Add a token to the response."""
        self.tokens.append(token)
        self.text += token
        
    def get_full_text(self) -> str:
        """Get the full text of the response."""
        return self.text
    
    def set_sources(self, sources: List[Dict[str, Any]]) -> None:
        """Set the sources used to generate the response."""
        self.sources = sources


class StreamingLLMChain:
    """
    Handler for streaming LLM responses token by token.
    
    This class interacts with the Ollama API to stream tokens
    as they're generated, allowing for immediate display to users.
    """
    
    def __init__(self, model_name: str = "mistral"):
        """
        Initialize the streaming LLM chain.
        
        Args:
            model_name: The name of the model to use (default: "mistral")
        """
        self.model_name = model_name
        self.ollama_base_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
        self.api_url = f"{self.ollama_base_url}/api/chat"
        self.max_tokens = 4000
        self.temperature = 0.7
        
    async def stream_chat(
        self, 
        message: str, 
        conversation_context: List[Dict[str, str]] = None,
        sources: List[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict[str, Union[str, bool]], None]:
        """
        Stream a chat response token by token.
        
        Args:
            message: The user's message to process
            conversation_context: Optional list of previous messages for context
            sources: Optional list of document sources for references
            
        Yields:
            Dictionary with token or completion status
        """
        try:
            # Prepare conversation context if provided
            messages = []
            
            # Add system message with instruction
            system_message = "You are a helpful assistant that provides accurate information."
            
            # Add source context if available
            if sources and len(sources) > 0:
                source_texts = []
                for idx, source in enumerate(sources, 1):
                    source_text = f"Source {idx}: {source.get('document', 'Unknown document')}\n"
                    content = source.get('content', '')
                    if content:
                        source_text += f"{content}\n"
                    source_texts.append(source_text)
                
                if source_texts:
                    context_str = "\n".join(source_texts)
                    system_message += "\n\nHere are some sources to help you provide an accurate response:\n" + context_str
                    system_message += "\n\nPlease use these sources to answer the user's question. If the sources don't contain the information needed, you can rely on your knowledge but make it clear that you're not using the provided sources."
            
            # Add system message
            messages.append({"role": "system", "content": system_message})
            
            # Add conversation history if provided
            if conversation_context:
                # Add up to 10 most recent messages to avoid exceeding context limits
                for msg in conversation_context[-10:]:
                    if "role" in msg and "content" in msg:
                        messages.append({"role": msg["role"], "content": msg["content"]})
            
            # Add the current user message
            messages.append({"role": "user", "content": message})
            
            # Prepare the request payload
            payload = {
                "model": self.model_name,
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": self.temperature
                }
            }
            
            # Create HTTP client with streaming support
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream("POST", self.api_url, json=payload) as response:
                    if response.status_code != 200:
                        error_detail = await response.aread()
                        error_msg = f"Ollama API error: {response.status_code} - {error_detail}"
                        logger.error(error_msg)
                        yield {"error": error_msg}
                        return
                    
                    # Stream the response
                    buffer = ""
                    async for chunk in response.aiter_text():
                        if not chunk.strip():
                            continue
                        
                        # Process each chunk, which might contain multiple JSON objects
                        buffer += chunk
                        while "\n" in buffer:
                            line, buffer = buffer.split("\n", 1)
                            try:
                                data = json.loads(line)
                                if "message" in data and "content" in data["message"]:
                                    token = data["message"]["content"]
                                    yield {"token": token}
                                elif "done" in data and data["done"]:
                                    # Final event indicating completion
                                    yield {"done": True}
                                    return
                            except json.JSONDecodeError:
                                logger.warning(f"Failed to decode JSON: {line}")
                    
                    # If there's any content left in the buffer
                    if buffer.strip():
                        try:
                            data = json.loads(buffer)
                            if "message" in data and "content" in data["message"]:
                                token = data["message"]["content"]
                                yield {"token": token}
                        except json.JSONDecodeError:
                            logger.warning(f"Failed to decode final JSON: {buffer}")
                    
                    # Final event
                    yield {"done": True}
        
        except httpx.RequestError as e:
            error_msg = f"Error connecting to Ollama API: {str(e)}"
            logger.error(error_msg)
            yield {"error": error_msg}
        
        except Exception as e:
            error_msg = f"Unexpected error in streaming: {str(e)}"
            logger.error(error_msg)
            yield {"error": error_msg}
    
    def check_ollama_status(self) -> Dict[str, Any]:
        """
        Check the status of the Ollama service.
        
        Returns:
            Dictionary with status information and available models
        """
        import requests
        
        try:
            # Check if Ollama is running
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            
            if response.status_code == 200:
                models_data = response.json()
                model_names = [model["name"] for model in models_data.get("models", [])]
                
                return {
                    "status": "available",
                    "models": model_names,
                    "current_model": self.model_name
                }
            else:
                return {
                    "status": "error",
                    "error": f"Ollama returned error: {response.status_code}",
                    "current_model": self.model_name
                }
                
        except requests.RequestException as e:
            return {
                "status": "unavailable",
                "error": f"Cannot connect to Ollama: {str(e)}",
                "current_model": self.model_name
            }
        except Exception as e:
            return {
                "status": "error",
                "error": f"Unexpected error checking Ollama status: {str(e)}",
                "current_model": self.model_name
            } 